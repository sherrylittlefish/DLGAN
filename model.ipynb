{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTNUTNtT44KFihpYPuld6+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sherrylittlefish/DLGAN/blob/main/model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9xZ5RuGmI9cb",
        "outputId": "ea002ce7-d320-45d6-f0b6-f02864e990a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**資料A**"
      ],
      "metadata": {
        "id": "1Nf-yUP7dran"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# 設定 zip 檔案路徑（根據你的實際位置修改）\n",
        "zip_path = '/content/drive/MyDrive/高雄大學/pic.zip'\n",
        "unzip_dir = '/content/unzipped'\n",
        "\n",
        "# 解壓 pic.zip 到 /content/unzipped\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_dir)\n"
      ],
      "metadata": {
        "id": "6dJ4qIlqXI3-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 解壓 filter1.zip\n",
        "filtered1_zip_path = os.path.join(unzip_dir, 'filtered1.zip')\n",
        "filtered1_dir = os.path.join(unzip_dir, 'filtered1')\n",
        "with zipfile.ZipFile(filtered1_zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(filtered1_dir)"
      ],
      "metadata": {
        "id": "J8_GB3trbEr3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "import random\n",
        "\n",
        "# 指定資料夾與輸出位置\n",
        "folders = ['0051', '0052', '0053']\n",
        "trainA_dir = '/content/dataset/trainA'\n",
        "testA_dir = '/content/dataset/testA'\n",
        "\n",
        "os.makedirs(trainA_dir, exist_ok=True)\n",
        "os.makedirs(testA_dir, exist_ok=True)\n",
        "\n",
        "# 收集所有圖片檔案路徑\n",
        "all_images = []\n",
        "for folder in folders:\n",
        "    folder_path = os.path.join(filtered1_dir, folder)\n",
        "    if os.path.isdir(folder_path):\n",
        "        for fname in os.listdir(folder_path):\n",
        "            fpath = os.path.join(folder_path, fname)\n",
        "            if os.path.isfile(fpath):\n",
        "                all_images.append(fpath)\n",
        "\n",
        "# 打亂順序\n",
        "random.shuffle(all_images)\n",
        "\n",
        "# 分割資料集（約 1:10 比例）\n",
        "split_index = max(1, int(len(all_images) * 0.1))  # 至少1張到testA\n",
        "test_images = all_images[:split_index]\n",
        "train_images = all_images[split_index:]\n",
        "\n",
        "# 複製圖片\n",
        "def copy_images(image_list, target_dir):\n",
        "    for src_path in image_list:\n",
        "        fname = os.path.basename(src_path)\n",
        "        dst_path = os.path.join(target_dir, fname)\n",
        "        # 若重名可加上 prefix 防止覆蓋（這裡為保簡單略過）\n",
        "        shutil.copy(src_path, dst_path)\n",
        "\n",
        "copy_images(train_images, trainA_dir)\n",
        "copy_images(test_images, testA_dir)\n",
        "\n",
        "print(f\"✔ 總共 {len(all_images)} 張圖片，已分成：\")\n",
        "print(f\"   ➤ trainA：{len(train_images)} 張\")\n",
        "print(f\"   ➤ testA ：{len(test_images)} 張\")\n"
      ],
      "metadata": {
        "id": "NOt0S6InbXm4",
        "outputId": "ea33f00f-05f2-48aa-9cc1-ffa95f4575ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ 總共 1359 張圖片，已分成：\n",
            "   ➤ trainA：1224 張\n",
            "   ➤ testA ：135 張\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**資料B**"
      ],
      "metadata": {
        "id": "Gz_3NyT3dj1u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path2 = '/content/drive/MyDrive/高雄大學/person_images_filtered.zip'\n",
        "# 解壓 person_images_filtered.zip 到 /content/unzipped\n",
        "with zipfile.ZipFile(zip_path2, 'r') as zip_ref:\n",
        "    zip_ref.extractall(unzip_dir)"
      ],
      "metadata": {
        "id": "Q1_aXGVvcC0T"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 原始圖片資料夾\n",
        "source_dir = '/content/unzipped/person_images_filtered'\n",
        "\n",
        "# 輸出目錄\n",
        "trainB_dir = '/content/dataset/trainB'\n",
        "testB_dir = '/content/dataset/testB'\n",
        "\n",
        "# 建立資料夾\n",
        "os.makedirs(trainB_dir, exist_ok=True)\n",
        "os.makedirs(testB_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# 取得所有圖片清單\n",
        "all_images = [f for f in os.listdir(source_dir) if os.path.isfile(os.path.join(source_dir, f))]\n",
        "\n",
        "# 打亂順序\n",
        "random.shuffle(all_images)\n",
        "\n",
        "# # 設定比例（約10%進 testB，其餘進 trainB）\n",
        "# num_test = max(1, int(len(all_images) * 0.1))  # 至少 1 張\n",
        "# test_images = all_images[:num_test]\n",
        "# train_images = all_images[num_test:]\n",
        "\n",
        "# 指定要切出來的數量\n",
        "num_train = 1200\n",
        "num_test = 150\n",
        "\n",
        "# 確保不超過總數\n",
        "num_train = min(num_train, len(all_images))\n",
        "num_test = min(num_test, len(all_images) - num_train)\n",
        "\n",
        "# 切分\n",
        "train_images = all_images[:num_train]\n",
        "test_images = all_images[num_train:num_train + num_test]\n",
        "\n",
        "# 複製檔案\n",
        "def copy_images(image_list, target_dir):\n",
        "    for fname in image_list:\n",
        "        src = os.path.join(source_dir, fname)\n",
        "        dst = os.path.join(target_dir, fname)\n",
        "        shutil.copy(src, dst)\n",
        "\n",
        "copy_images(train_images, trainB_dir)\n",
        "copy_images(test_images, testB_dir)\n",
        "\n",
        "print(f\"✔ 已完成分割：\")\n",
        "print(f\"   ➤ trainB：{len(train_images)} 張\")\n",
        "print(f\"   ➤ testB ：{len(test_images)} 張\")\n"
      ],
      "metadata": {
        "id": "oFzY7krKdMfc",
        "outputId": "4a98ec1b-59fe-45a2-f3c4-35225ca8ad91",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✔ 已完成分割：\n",
            "   ➤ trainB：1200 張\n",
            "   ➤ testB ：150 張\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 殘差區塊\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, kernel_size=3),\n",
        "            nn.InstanceNorm2d(dim),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.ReflectionPad2d(1),\n",
        "            nn.Conv2d(dim, dim, kernel_size=3),\n",
        "            nn.InstanceNorm2d(dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.block(x)\n",
        "\n",
        "# Generator 架構\n",
        "class ResnetGenerator(nn.Module):\n",
        "    def __init__(self, input_nc, output_nc, ngf=64, n_blocks=9):\n",
        "        super(ResnetGenerator, self).__init__()\n",
        "\n",
        "        model = [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(input_nc, ngf, kernel_size=7),\n",
        "            nn.InstanceNorm2d(ngf),\n",
        "            nn.ReLU(inplace=True),\n",
        "        ]\n",
        "\n",
        "        # Downsampling\n",
        "        in_features = ngf\n",
        "        out_features = in_features * 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features *= 2\n",
        "\n",
        "        # Residual Blocks\n",
        "        for _ in range(n_blocks):\n",
        "            model += [ResidualBlock(in_features)]\n",
        "\n",
        "        # Upsampling\n",
        "        out_features = in_features // 2\n",
        "        for _ in range(2):\n",
        "            model += [\n",
        "                nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "                nn.InstanceNorm2d(out_features),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ]\n",
        "            in_features = out_features\n",
        "            out_features = in_features // 2\n",
        "\n",
        "        # Output layer\n",
        "        model += [\n",
        "            nn.ReflectionPad2d(3),\n",
        "            nn.Conv2d(in_features, output_nc, kernel_size=7),\n",
        "            nn.Tanh()\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "O61GxyEtgRai"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class PatchDiscriminator(nn.Module):\n",
        "    def __init__(self, input_nc, ndf=64, n_layers=3):\n",
        "        super(PatchDiscriminator, self).__init__()\n",
        "\n",
        "        # 第一層：不使用 InstanceNorm\n",
        "        layers = [\n",
        "            nn.Conv2d(input_nc, ndf, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        # 中間層\n",
        "        nf_mult = 1\n",
        "        for n in range(1, n_layers):\n",
        "            nf_mult_prev = nf_mult\n",
        "            nf_mult = min(2**n, 8)\n",
        "            layers += [\n",
        "                nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
        "                          kernel_size=4, stride=2, padding=1),\n",
        "                nn.InstanceNorm2d(ndf * nf_mult),\n",
        "                nn.LeakyReLU(0.2, inplace=True)\n",
        "            ]\n",
        "\n",
        "        # 最後一層（不 downsample）\n",
        "        nf_mult_prev = nf_mult\n",
        "        nf_mult = min(2**n_layers, 8)\n",
        "        layers += [\n",
        "            nn.Conv2d(ndf * nf_mult_prev, ndf * nf_mult,\n",
        "                      kernel_size=4, stride=1, padding=1),\n",
        "            nn.InstanceNorm2d(ndf * nf_mult),\n",
        "            nn.LeakyReLU(0.2, inplace=True)\n",
        "        ]\n",
        "\n",
        "        # 輸出層：1 channel 的「真假 map」\n",
        "        layers += [\n",
        "            nn.Conv2d(ndf * nf_mult, 1, kernel_size=4, stride=1, padding=1)\n",
        "        ]\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "uHx47v0Hgd0-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 判別器 loss 用 LSGAN\n",
        "class GANLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GANLoss, self).__init__()\n",
        "        self.loss = nn.MSELoss()\n",
        "\n",
        "    def get_target_tensor(self, prediction, target_is_real):\n",
        "        if target_is_real:\n",
        "            return torch.ones_like(prediction)\n",
        "        else:\n",
        "            return torch.zeros_like(prediction)\n",
        "\n",
        "    def forward(self, prediction, target_is_real):\n",
        "        target_tensor = self.get_target_tensor(prediction, target_is_real)\n",
        "        return self.loss(prediction, target_tensor)\n"
      ],
      "metadata": {
        "id": "PTGapuywhJJu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 構造 loss function\n",
        "criterion_GAN = GANLoss()\n",
        "criterion_cycle = nn.L1Loss()\n",
        "criterion_identity = nn.L1Loss()  # 可選\n"
      ],
      "metadata": {
        "id": "4sfjyAnZhO3t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "G_AB = ResnetGenerator(input_nc=3, output_nc=3)\n",
        "G_BA = ResnetGenerator(input_nc=3, output_nc=3)\n",
        "D_A = PatchDiscriminator(input_nc=3)\n",
        "D_B = PatchDiscriminator(input_nc=3)"
      ],
      "metadata": {
        "id": "MON8bhMghZKg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 假設你已經定義：\n",
        "# G_AB = A -> B Generator\n",
        "# G_BA = B -> A Generator\n",
        "# D_A = 判別 A 圖\n",
        "# D_B = 判別 B 圖\n",
        "\n",
        "# 優化器\n",
        "import torch.optim as optim\n",
        "\n",
        "lr = 0.0002\n",
        "G_params = list(G_AB.parameters()) + list(G_BA.parameters())\n",
        "optimizer_G = optim.Adam(G_params, lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_A = optim.Adam(D_A.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "optimizer_D_B = optim.Adam(D_B.parameters(), lr=lr, betas=(0.5, 0.999))\n"
      ],
      "metadata": {
        "id": "fSldNWM2hRej"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_step(real_A, real_B):\n",
        "    ###### 1. 訓練 Generator ######\n",
        "    optimizer_G.zero_grad()\n",
        "\n",
        "    # A -> B\n",
        "    fake_B = G_AB(real_A)\n",
        "    pred_fake_B = D_B(fake_B)\n",
        "    loss_GAN_AB = criterion_GAN(pred_fake_B, True)\n",
        "\n",
        "    # B -> A\n",
        "    fake_A = G_BA(real_B)\n",
        "    pred_fake_A = D_A(fake_A)\n",
        "    loss_GAN_BA = criterion_GAN(pred_fake_A, True)\n",
        "\n",
        "    # Cycle consistency loss\n",
        "    recovered_A = G_BA(fake_B)\n",
        "    recovered_B = G_AB(fake_A)\n",
        "    loss_cycle_A = criterion_cycle(recovered_A, real_A)\n",
        "    loss_cycle_B = criterion_cycle(recovered_B, real_B)\n",
        "\n",
        "    # Identity loss (optional)\n",
        "    same_A = G_BA(real_A)\n",
        "    same_B = G_AB(real_B)\n",
        "    loss_idt_A = criterion_identity(same_A, real_A)\n",
        "    loss_idt_B = criterion_identity(same_B, real_B)\n",
        "\n",
        "    # 加總 loss\n",
        "    lambda_cycle = 10.0\n",
        "    lambda_id = 5.0\n",
        "    loss_G = (\n",
        "        loss_GAN_AB + loss_GAN_BA +\n",
        "        lambda_cycle * (loss_cycle_A + loss_cycle_B) +\n",
        "        lambda_id * (loss_idt_A + loss_idt_B)\n",
        "    )\n",
        "\n",
        "    loss_G.backward()\n",
        "    optimizer_G.step()\n",
        "\n",
        "    ###### 2. 訓練 Discriminator A ######\n",
        "    optimizer_D_A.zero_grad()\n",
        "    pred_real_A = D_A(real_A)\n",
        "    pred_fake_A = D_A(fake_A.detach())\n",
        "    loss_D_A = (\n",
        "        criterion_GAN(pred_real_A, True) +\n",
        "        criterion_GAN(pred_fake_A, False)\n",
        "    ) * 0.5\n",
        "    loss_D_A.backward()\n",
        "    optimizer_D_A.step()\n",
        "\n",
        "    ###### 3. 訓練 Discriminator B ######\n",
        "    optimizer_D_B.zero_grad()\n",
        "    pred_real_B = D_B(real_B)\n",
        "    pred_fake_B = D_B(fake_B.detach())\n",
        "    loss_D_B = (\n",
        "        criterion_GAN(pred_real_B, True) +\n",
        "        criterion_GAN(pred_fake_B, False)\n",
        "    ) * 0.5\n",
        "    loss_D_B.backward()\n",
        "    optimizer_D_B.step()\n",
        "\n",
        "    return {\n",
        "        \"loss_G\": loss_G.item(),\n",
        "        \"loss_D_A\": loss_D_A.item(),\n",
        "        \"loss_D_B\": loss_D_B.item()\n",
        "    }\n"
      ],
      "metadata": {
        "id": "vo1BzU4yhrJh"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "# 自訂 Dataset\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root_A, root_B, transform=None):\n",
        "        self.files_A = sorted([os.path.join(root_A, f) for f in os.listdir(root_A)])\n",
        "        self.files_B = sorted([os.path.join(root_B, f) for f in os.listdir(root_B)])\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return min(len(self.files_A), len(self.files_B))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_A = Image.open(self.files_A[index % len(self.files_A)]).convert('RGB')\n",
        "        img_B = Image.open(self.files_B[index % len(self.files_B)]).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            img_A = self.transform(img_A)\n",
        "            img_B = self.transform(img_B)\n",
        "\n",
        "        return {'A': img_A, 'B': img_B}\n",
        "\n",
        "# 轉換設定（可微調）\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # [-1, 1]\n",
        "])\n",
        "\n",
        "# 路徑設定\n",
        "train_dataset = ImageDataset('/content/dataset/trainA', '/content/dataset/trainB', transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n"
      ],
      "metadata": {
        "id": "SC0zmQVshwIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def denormalize(tensor):\n",
        "    return tensor * 0.5 + 0.5  # 把 [-1,1] 還原到 [0,1]\n",
        "\n",
        "def visualize_output(real_A, fake_B, real_B, fake_A):\n",
        "    images = [real_A, fake_B, real_B, fake_A]\n",
        "    titles = ['Real A', 'Fake B (A→B)', 'Real B', 'Fake A (B→A)']\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    for i, img in enumerate(images):\n",
        "        img = denormalize(img[0]).permute(1, 2, 0).cpu().detach().numpy()\n",
        "        plt.subplot(1, 4, i+1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(titles[i])\n",
        "        plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "qdnzSB21h2cR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "writer = SummaryWriter(log_dir='runs/cyclegan_experiment')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "G_AB.to(device)\n",
        "G_BA.to(device)\n",
        "D_A.to(device)\n",
        "D_B.to(device)\n",
        "\n",
        "# 訓練迴圈\n",
        "n_epochs = 10\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    for i, data in enumerate(train_loader):\n",
        "        real_A = data['A'].to(device)\n",
        "        real_B = data['B'].to(device)\n",
        "\n",
        "        losses = train_step(real_A, real_B)\n",
        "\n",
        "        writer.add_scalar(\"Loss/G\", losses['loss_G'], epoch * len(train_loader) + i)\n",
        "        writer.add_scalar(\"Loss/D_A\", losses['loss_D_A'], epoch * len(train_loader) + i)\n",
        "        writer.add_scalar(\"Loss/D_B\", losses['loss_D_B'], epoch * len(train_loader) + i)\n",
        "\n",
        "\n",
        "        if i % 50 == 0:\n",
        "            print(f\"[Epoch {epoch}/{n_epochs}] [Batch {i}/{len(train_loader)}] \"\n",
        "                  f\"Loss_G: {losses['loss_G']:.4f}, \"\n",
        "                  f\"Loss_DA: {losses['loss_D_A']:.4f}, \"\n",
        "                  f\"Loss_DB: {losses['loss_D_B']:.4f}\")\n",
        "        if i % 200 == 0:\n",
        "            with torch.no_grad():\n",
        "                fake_B = G_AB(real_A)\n",
        "                fake_A = G_BA(real_B)\n",
        "                visualize_output(real_A, fake_B, real_B, fake_A)\n",
        "    # 每 N epoch 儲存一次\n",
        "    if epoch % 2 == 0:\n",
        "        torch.save(G_AB.state_dict(), f'G_AB_epoch{epoch}.pth')\n",
        "        torch.save(G_BA.state_dict(), f'G_BA_epoch{epoch}.pth')\n",
        "        torch.save(D_A.state_dict(), f'D_A_epoch{epoch}.pth')\n",
        "        torch.save(D_B.state_dict(), f'D_B_epoch{epoch}.pth')\n",
        "        print(f\"✅ 已儲存模型於 epoch {epoch}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aeqfTyz_hzMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs\n"
      ],
      "metadata": {
        "id": "JtkWoLlwiiHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.utils import save_image\n",
        "\n",
        "def test_sample(model_G, input_dir, output_dir, transform):\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    files = sorted(os.listdir(input_dir))\n",
        "    model_G.eval()\n",
        "\n",
        "    for fname in files:\n",
        "        path = os.path.join(input_dir, fname)\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            fake = model_G(img_tensor)\n",
        "\n",
        "        out_img = denormalize(fake[0]).cpu()\n",
        "        save_image(out_img, os.path.join(output_dir, fname))\n",
        "    print(f\"✅ 測試圖片儲存於：{output_dir}\")\n"
      ],
      "metadata": {
        "id": "H1gjpqpYilC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sample(G_AB, '/content/dataset/testA', '/content/testA2B_output', transform)\n",
        "test_sample(G_BA, '/content/dataset/testB', '/content/testB2A_output', transform)\n"
      ],
      "metadata": {
        "id": "y93aLye0imh5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}